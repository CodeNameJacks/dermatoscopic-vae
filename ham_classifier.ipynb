{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8abf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from config import Config as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ef1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_dataset_dir = 'ham'\n",
    "batch_size = 48\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4197b01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd97b703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10015 files belonging to 7 classes.\n",
      "Using 8012 files for training.\n",
      "Found 10015 files belonging to 7 classes.\n",
      "Using 2003 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.preprocessing.image_dataset_from_directory(ham_dataset_dir, validation_split=0.2, color_mode='rgb',\n",
    "                                                            labels='inferred', shuffle=True, subset='training', image_size=(256, 256),\n",
    "                                                            batch_size=batch_size, seed=seed)\n",
    "\n",
    "val_ds = keras.preprocessing.image_dataset_from_directory(ham_dataset_dir, validation_split=0.2, color_mode='rgb',\n",
    "                                                          labels='inferred', shuffle=True, subset='validation', image_size=(256, 256),\n",
    "                                                          batch_size=batch_size, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f06af64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10015 files belonging to 7 classes.\n",
      "Using 8012 files for training.\n"
     ]
    }
   ],
   "source": [
    "classes = train_ds.class_names\n",
    "\n",
    "train_ds_single_batch = keras.preprocessing.image_dataset_from_directory(ham_dataset_dir, validation_split=0.2, color_mode='rgb',\n",
    "                                                                         labels='inferred', shuffle=True, subset='training', image_size=(256, 256),\n",
    "                                                                         batch_size=1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fec501f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad25302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.31913747  2.77135939  1.31107838 12.86035313  1.26611884  0.2138986\n",
      "  9.78266178]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([label.numpy()[0] for _, label in train_ds_single_batch])\n",
    "class_weights_list = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "print(class_weights_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fcd5e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "len_train = np.array([label.numpy()[0] for _, label in train_ds])\n",
    "steps_per_epoch = len(len_train)//batch_size\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "332060ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "validation_steps = len(val_ds)//batch_size\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e00b5af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {}\n",
    "for i in range(len(class_weights_list)):\n",
    "    class_weights[i] = class_weights_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b61820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale = keras.layers.experimental.preprocessing.Rescaling(scale=1.0 / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1f2e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda x, y: (rescale(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (rescale(x), y))\n",
    "\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "155f6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ResNet-18\n",
    "Reference:\n",
    "[1] K. He et al. Deep Residual Learning for Image Recognition. CVPR, 2016\n",
    "[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:\n",
    "Surpassing human-level performance on imagenet classification. In\n",
    "ICCV, 2015.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08246f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(Model):\n",
    "    \"\"\"\n",
    "    A standard resnet block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, down_sample=False):\n",
    "        \"\"\"\n",
    "        channels: same as number of convolution kernels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.__channels = channels\n",
    "        self.__down_sample = down_sample\n",
    "        self.__strides = [2, 1] if down_sample else [1, 1]\n",
    "\n",
    "        KERNEL_SIZE = (3, 3)\n",
    "        # use He initialization, instead of Xavier (a.k.a 'glorot_uniform' in Keras), as suggested in [2]\n",
    "        INIT_SCHEME = \"he_normal\"\n",
    "\n",
    "        self.conv_1 = Conv2D(self.__channels, strides=self.__strides[0],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        \n",
    "        self.bn_1 = BatchNormalization()\n",
    "        self.conv_2 = Conv2D(self.__channels, strides=self.__strides[1],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_2 = BatchNormalization()\n",
    "        self.merge = Add()\n",
    "\n",
    "        if self.__down_sample:\n",
    "            # perform down sampling using stride of 2, according to [1].\n",
    "            self.res_conv = Conv2D(\n",
    "                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n",
    "            self.res_bn = BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        res = inputs\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.bn_1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "\n",
    "        if self.__down_sample:\n",
    "            res = self.res_conv(res)\n",
    "            res = self.res_bn(res)\n",
    "\n",
    "        # if not perform down sample, then add a shortcut directly\n",
    "        x = self.merge([x, res])\n",
    "        out = tf.nn.relu(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "859fe29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(Model):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"\n",
    "            num_classes: number of classes in specific classification task.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_1 = Conv2D(64, (7, 7), strides=2,\n",
    "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        self.init_bn = BatchNormalization()\n",
    "        self.pool_2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
    "        self.res_1_1 = ResnetBlock(64)\n",
    "        self.res_1_2 = ResnetBlock(64)\n",
    "        self.res_2_1 = ResnetBlock(128, down_sample=True)\n",
    "        self.res_2_2 = ResnetBlock(128)\n",
    "        self.res_3_1 = ResnetBlock(256, down_sample=True)\n",
    "        self.res_3_2 = ResnetBlock(256)\n",
    "        self.res_4_1 = ResnetBlock(512, down_sample=True)\n",
    "        self.res_4_2 = ResnetBlock(512)\n",
    "        self.avg_pool = GlobalAveragePooling2D()\n",
    "        self.flat = Flatten()\n",
    "        self.fc = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.conv_1(inputs)\n",
    "        out = self.init_bn(out)\n",
    "        out = tf.nn.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2, self.res_4_1, self.res_4_2]:\n",
    "            out = res_block(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flat(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28efd1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"res_net18_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_120 (Conv2D)         multiple                  9472      \n",
      "                                                                 \n",
      " batch_normalization_120 (Ba  multiple                 256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " resnet_block_48 (ResnetBloc  multiple                 74368     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " resnet_block_49 (ResnetBloc  multiple                 74368     \n",
      " k)                                                              \n",
      "                                                                 \n",
      " resnet_block_50 (ResnetBloc  multiple                 231296    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " resnet_block_51 (ResnetBloc  multiple                 296192    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " resnet_block_52 (ResnetBloc  multiple                 921344    \n",
      " k)                                                              \n",
      "                                                                 \n",
      " resnet_block_53 (ResnetBloc  multiple                 1182208   \n",
      " k)                                                              \n",
      "                                                                 \n",
      " resnet_block_54 (ResnetBloc  multiple                 3677696   \n",
      " k)                                                              \n",
      "                                                                 \n",
      " resnet_block_55 (ResnetBloc  multiple                 4723712   \n",
      " k)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d_6   multiple                 0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             multiple                  3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,194,503\n",
      "Trainable params: 11,184,903\n",
      "Non-trainable params: 9,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ResNet18(7)\n",
    "model.build(input_shape = (None,32,32,3))\n",
    "#use categorical_crossentropy since the label is one-hot encoded\n",
    "# from keras.optimizers import SGD\n",
    "# opt = SGD(learning_rate=0.1,momentum=0.9,decay = 1e-04) #parameters suggested by He [1]\n",
    "model.compile(optimizer = \"adam\",loss='categorical_crossentropy', metrics=[\"accuracy\"]) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbf42ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\engine\\training.py\", line 946, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\engine\\training.py\", line 935, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\engine\\training.py\", line 928, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\engine\\training.py\", line 842, in train_step\n        loss = self.compiled_loss(\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\backend.py\", line 5043, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 7) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8976/2288088569.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mSTEPS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# history = model.fit(train_ds, steps_per_epoch=STEPS, batch_size = batch_size, epochs=50, validation_data=val_ds,callbacks=[es])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\engine\\training.py\", line 946, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\engine\\training.py\", line 935, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\engine\\training.py\", line 928, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\engine\\training.py\", line 842, in train_step\n        loss = self.compiled_loss(\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\alif-\\anaconda3\\envs\\tf2.5\\lib\\site-packages\\keras\\backend.py\", line 5043, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 7) are incompatible\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# es = EarlyStopping(patience= 8, restore_best_weights=True, monitor=\"val_acc\")\n",
    "#I did not use cross validation, so the validate performance is not accurate.\n",
    "STEPS = len(y) // batch_size\n",
    "# history = model.fit(train_ds, steps_per_epoch=STEPS, batch_size = batch_size, epochs=50, validation_data=val_ds,callbacks=[es])\n",
    "history = model.fit(train_ds, steps_per_epoch=STEPS, batch_size = batch_size, epochs=50, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8122f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8012//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "143badf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8300"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "166*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "838856f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "166/166 [==============================] - 25s 150ms/step - loss: 0.0000e+00 - accuracy: 0.0517 - val_loss: 0.0000e+00 - val_accuracy: 0.0508\n",
      "Epoch 2/20\n",
      "  1/166 [..............................] - ETA: 19s - loss: 0.0000e+00 - accuracy: 0.0227WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3320 batches). You may need to use the repeat() function when building your dataset.\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0000e+00 - accuracy: 0.0227 - val_loss: 0.0000e+00 - val_accuracy: 0.0503\n"
     ]
    }
   ],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# es = EarlyStopping(patience= 8, restore_best_weights=True, monitor=\"val_acc\")\n",
    "#I did not use cross validation, so the validate performance is not accurate.\n",
    "# STEPS = len(y) // batch_size\n",
    "# history = model.fit(train_ds, steps_per_epoch=STEPS, batch_size = batch_size, epochs=50, validation_data=val_ds,callbacks=[es])\n",
    "history = model.fit(train_ds, \n",
    "                    steps_per_epoch= 8012//batch_size, \n",
    "                    epochs=20, \n",
    "                    validation_data=val_ds, \n",
    "                    validation_steps=2003//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed42eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
